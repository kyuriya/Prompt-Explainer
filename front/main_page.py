import streamlit as st
from back.llm_service import get_huggingface_response
from back.chat_storage import save_chat_history
from back.captum_utils import generate_heatmap  # Utility for Captum heatmap
from io import BytesIO
from PIL import Image
import torch
import gc
from datetime import datetime
import os
import base64
import json
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
# ì•Œê³ ë¦¬ì¦˜ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
ALGORITHM_PROMPTS = {
    "Depth-First Search(DFS)": """
            You are an algorithm implementation expert.
            The user wants to implement the Depth-First Search (DFS) algorithm.
            Follow these rules:
                1.	Write the DFS algorithm in Python.
                2.	The DFS algorithm should operate based on a graph represented using an adjacency list.
                3.	Start the traversal from the given start node and return the visited nodes in order.
                4.	Name the function dfs_traversal and ensure it takes the following parameters:
                â€¢	graph: A graph in adjacency list format (dictionary).
                â€¢	start: The node where the traversal should begin.
                5.	The function should return a list of nodes in the order they were visited.
                6.	The input graph can either be a connected graph or a disconnected graph.
                7.	Add simple comments to explain the key steps in the code.

            Additionally, consider the following:
                â€¢	Ensure visited nodes are not processed more than once.
                â€¢	If the input graph is empty, return an empty list.
                â€¢	Choose either a recursive or stack-based implementation.
            """,
    "Breadth-First Search(BFS)": """
            You are an algorithm implementation expert.
            The user wants to implement the Breadth-First Search (BFS) algorithm.
            Follow these rules:
                1.	Write the BFS algorithm in Python.
                2.	The BFS algorithm should operate based on a graph represented using an adjacency list.
                3.	Start the traversal from the given start node and return the visited nodes in order.
                4.	Name the function bfs_traversal and ensure it takes the following parameters:
                â€¢	graph: A graph in adjacency list format (dictionary).
                â€¢	start: The node where the traversal should begin.
                5.	The function should return a list of nodes in the order they were visited.
                6.	The input graph can either be a connected graph or a disconnected graph.
                7.	Add simple comments to explain the key steps in the code.

            Additionally, consider the following:
                â€¢	Ensure visited nodes are not processed more than once.
                â€¢	If the input graph is empty, return an empty list.
                â€¢	Use a queue to implement the traversal process.
            """ ,
    "Sort Algorithm": """
            You are an algorithm implementation expert.
            The user wants to implement a sorting algorithm.
            Follow these rules:
                1.	Write the sorting algorithm in Python.
                2.	The user wants to implement a specific sorting algorithm (e.g., Bubble Sort, Quick Sort, Merge Sort, etc.).
                3.	The sorting algorithm implementation must follow these rules:
                â€¢	The function name should be sort_algorithm and should take the list to be sorted as a parameter.
                â€¢	The function must return the sorted list.
                4.	The function should perform sorting in ascending order by default.
                5.	Add simple comments to explain the key steps of the algorithm.

            Additionally, consider the following:
                â€¢	If the input list is empty or contains only one element, return it as is.
                â€¢	Choose and implement an appropriate sorting algorithm (e.g., Bubble Sort using basic loops).
            """,
    "Greedy Algorithm": """
            You are an algorithm implementation expert.
            The user wants to implement a Greedy Algorithm.
            Follow these rules:
                1.	Write the Greedy Algorithm in Python.
                2.	Solve a specific problem type (e.g., Activity Selection Problem, Minimum Spanning Tree, Coin Change Problem) based on the user's requirements.
                3.	The implemented function should include:
                â€¢	The function name and parameters, defined dynamically based on the problem type.
                â€¢	A clear explanation of the greedy criterion used for selection (e.g., maximum, minimum, etc.).
                4.	Include comments explaining the key steps of the algorithm.

            Additionally, consider the following:
                â€¢	Clearly specify when the greedy algorithm guarantees an optimal solution.
                â€¢	Include exception handling for problem input values.
            """,
    "Dynamic Programming(DP)": """
            You are an algorithm implementation expert.
            The user wants to solve a problem using Dynamic Programming (DP).
            Follow these rules:
                1.	Write the Dynamic Programming solution in Python.
                2.	Solve a specific problem type (e.g., Fibonacci sequence, Knapsack problem, Shortest Path, etc.) based on the user's requirements.
                3.	The implemented function should include:
                â€¢	A function name and parameters defined dynamically based on the problem type.
                â€¢	Use either the memoization or tabulation approach for the DP solution.
                4.	Include comments explaining the key steps and the structure of the DP table.

            Additionally, consider the following:
                â€¢	Clearly implement the recurrence relation for the problem.
                â€¢	Ensure optimized time complexity.
                â€¢	Handle cases where the input data is empty or invalid with exception handling.
            """,
    "Short Distance Algorithm": """
            You are an algorithm implementation expert.
            The user wants to implement a shortest path algorithm.
            Follow these rules:
                1.	Write the shortest path algorithm in Python.
                2.	The algorithm to be implemented should be one of the following: Dijkstra, Floyd-Warshall, or Bellman-Ford.
                3.	The implemented function must adhere to the following rules:
                â€¢	Name the function shortest_path, and take the input graph and starting node as parameters.
                â€¢	The graph should be represented as an adjacency list or a weighted matrix.
                â€¢	Return the shortest distance values for each node.
                4.	Include comments explaining the key steps of the algorithm.

            Additionally, consider the following:
                â€¢	Handle cases where the graph is empty by implementing appropriate exception handling.
                â€¢	Optimize the time complexity of the implemented algorithm.
            """
}
# GPU ë©”ëª¨ë¦¬ ë° ìºì‹œ ì´ˆê¸°í™” í•¨ìˆ˜
def clear_gpu_cache():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()
def render_main_page():
    clear_gpu_cache()
    """ì¤‘ì•™ ë©”ì¸ í˜ì´ì§€ êµ¬í˜„"""
    st.header(f"Conversation {st.session_state['current_page']}")

    # ì•ˆë‚´ë¬¸êµ¬ í‘œì‹œ (ì²˜ìŒ í•œ ë²ˆë§Œ)
    if not st.session_state.get("greetings", False):
        with st.chat_message("assistant"):
            intro = """
            Welcome to **Prompt Explainer**! ğŸ¤µğŸ»â€â™€ï¸\n
            This tool is designed to help you leverage LLMs (Large Language Models) more effectively when **solving algorithm problems**. â›³ï¸\n
            By visually highlighting which **parts of the prompt the LLM focuses on**, you can craft **better prompts** and receive **higher-quality response codes**. ğŸ²\n
            When you input a prompt, we will visualize the emphasized sections based on **SHAP values**. This allows you to learn better **prompt-writing strategies** and **maximize the utility of LLMs** in your workflow. ğŸï¸\n 
            Give it a try and enhance your experience in solving algorithmic problems! ğŸ¸
            """
            st.markdown(intro)
            st.session_state.messages.append({"role": "assistant", "content": intro})
        st.session_state.greetings = True
        st.rerun()

    # ìƒíƒœ ê´€ë¦¬: ë²„íŠ¼ì´ ëˆŒë¦¬ì§€ ì•Šì•˜ì„ ë•Œ
    if "button_pressed" not in st.session_state:
        st.session_state.button_pressed = None
        st.session_state.system_prompt = None

    # ëŒ€í™” ë©”ì‹œì§€ ì¶œë ¥
    for message in st.session_state["messages"]:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if message["role"] == "assistant" and "heatmap" in message:
                # base64 ë¬¸ìì—´ì„ ì´ë¯¸ì§€ ë°ì´í„°ë¡œ ë³€í™˜
                image_bytes = base64.b64decode(message["heatmap"])
                st.image(image_bytes)  # ë°”ì´íŠ¸ ë°ì´í„°ë¡œ ì§ì ‘ ì „ë‹¬

    # ì•Œê³ ë¦¬ì¦˜ ë²„íŠ¼ ì¶œë ¥
    if "button_pressed" not in st.session_state:
        st.session_state.button_pressed = None
        st.session_state.system_prompt = None

    # ë²„íŠ¼ì´ ëˆŒë¦¬ì§€ ì•Šì•˜ì„ ë•Œ ì•Œê³ ë¦¬ì¦˜ ë²„íŠ¼ ì¶œë ¥
    if st.session_state.button_pressed is None:
        cols = st.columns(3)
        for idx, (algo, prompt) in enumerate(ALGORITHM_PROMPTS.items()):
            with cols[idx % 3]:
                if st.button(algo):
                    st.session_state.button_pressed = algo
                    st.session_state.system_prompt = prompt
                    st.rerun()

    # ë²„íŠ¼ì´ ëˆŒë¦° í›„ ì„ íƒëœ ì•Œê³ ë¦¬ì¦˜ì˜ í”„ë¡¬í”„íŠ¸ í‘œì‹œ
    else:
        st.sidebar.title("System Prompt")
        st.sidebar.info(st.session_state.system_prompt)  # ì‚¬ì´ë“œë°”ì— ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í‘œì‹œ

        # ì„ íƒëœ ì•Œê³ ë¦¬ì¦˜ì˜ ìƒíƒœì—ì„œ "ë‹¤ì‹œ ì„ íƒ" ë²„íŠ¼ì„ ë§Œë“¤ì–´ ìƒíƒœ ì´ˆê¸°í™”
        col = st.columns(1)[0]  # ì¤‘ì•™ ì •ë ¬ì„ ìœ„í•´ 1ì—´ ì‚¬ìš©
        with col:
            if st.button(st.session_state.button_pressed, key="selected_button"):
                st.session_state.button_pressed = None  # ìƒíƒœ ì´ˆê¸°í™”
                st.session_state.system_prompt = None

        # "ë‹¤ì‹œ ì„ íƒ" ë²„íŠ¼ì„ ëˆŒëŸ¬ ì„ íƒì„ ì´ˆê¸°í™”í•˜ëŠ” ê¸°ëŠ¥
        if st.button("ë‹¤ì‹œ ì„ íƒ"):
            st.session_state.button_pressed = None  # ìƒíƒœ ì´ˆê¸°í™”
            st.session_state.system_prompt = None
            st.rerun()  # ë¦¬í”„ë ˆì‹œí•˜ì—¬ ë‹¤ì‹œ ì²˜ìŒ ìƒíƒœë¡œ ëŒì•„ê°€ê¸°

    # í”„ë¡¬í”„íŠ¸ ì…ë ¥ ì°½ (st.chat_input() ì‚¬ìš©)
    user_input = st.chat_input("Your prompt:")
    # if user_input:  # ì‚¬ìš©ìê°€ ì…ë ¥ì„ í•˜ë©´
    #     if user_input.strip():
    #         try:
    #             response = get_huggingface_response(st.session_state["model"], user_input)
    #             if response:
    #                 # ë©”ì‹œì§€ ê¸°ë¡ ì¶”ê°€
    #                 st.session_state.messages.append({"role": "user", "content": user_input})
    #                 st.session_state.messages.append({"role": "assistant", "content": response})
                # ì‚¬ìš©ì ì…ë ¥ ë©”ì‹œì§€ ì¦‰ì‹œ í‘œì‹œ
                # st.session_state.messages.append({"role": "user", "content": user_input})
                
                # LLM ì‘ë‹µ ìƒì„±
                # response = get_huggingface_response(st.session_state["model"], user_input)
                
                # if response:
                    # ì‘ë‹µ ë©”ì‹œì§€ ì¦‰ì‹œ í‘œì‹œ
                    # st.session_state.messages.append({"role": "assistant", "content": response})
                    
                    # íˆíŠ¸ë§µ ìƒì„±
                    # with st.spinner('Generating attribution heatmap...'):
                    #     heatmap_buffer = generate_heatmap(st.session_state["model"], user_input, response)
                        
                    #     if heatmap_buffer:
                    #         # íˆíŠ¸ë§µ ì´ë¯¸ì§€ ì €ì¥
                    #         timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    #         save_path = f"heatmaps/heatmap_{timestamp}.png"
                    #         os.makedirs("heatmaps", exist_ok=True)
                            
                    #         # BytesIOë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ê³  ì €ì¥
                    #         heatmap_image = Image.open(heatmap_buffer)
                    #         heatmap_image.save(save_path)
                        
            #         # íˆíŠ¸ë§µ í‘œì‹œ
            #         st.image(heatmap_image, caption="Prompt Attribution Heatmap", use_column_width=True)
                    
            #         # ëŒ€í™” ê¸°ë¡ ì €ì¥
            #         chat_history = st.session_state["chat_history"]
            #         current_page = st.session_state["current_page"] - 1
                    
            #         if current_page < len(chat_history):
            #             chat_history[current_page] = {"messages": st.session_state["messages"]}
            #         else:
            #             chat_history.append({"messages": st.session_state["messages"]})

            #         save_chat_history(chat_history)
                    
            #         st.rerun()

            # except Exception as e:
            #     st.error(f"An error occurred: {str(e)}")
        
    # if user_input:  # ì‚¬ìš©ìê°€ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ë©´
    #     # if user_input.strip():
    #     if user_input is not None and user_input.strip():
    #         try:
    #             response_lines = get_huggingface_response(st.session_state["model"], user_input)
    #             if response_lines:
    #                 # ë©”ì‹œì§€ ê¸°ë¡ ì¶”ê°€
    #                 st.session_state.messages.append({"role": "user", "content": user_input})
    #                 st.session_state.messages.append({"role": "assistant", "content": "\n".join(response_lines)})

    #                 # Captum ê¸°ì—¬ë„ ê³„ì‚°
    #                 json_path = generate_heatmap(st.session_state["model"], user_input, response_lines)

    #                 if json_path:
    #                     # JSON íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ íˆíŠ¸ë§µ ìƒì„± ë° í‘œì‹œ
    #                     st.markdown("### Prompt Attribution Heatmap")
    #                     with open(json_path, "r") as json_file:
    #                         attribution_data = json.load(json_file)
    #                     # ê° ì¤„ì˜ íˆíŠ¸ë§µ ìƒì„± (ì—¬ê¸°ì„œ êµ¬í˜„ í•„ìš”)
    #                     for idx, data in attribution_data.items():
    #                         st.markdown(f"**{data['line']}**")
    #                         # íˆíŠ¸ë§µ ì‹œê°í™” ì½”ë“œ ì‚½ì…
    #                         st.image(f"heatmaps/heatmap_{idx}.png", use_column_width=True)

    #                 # save_chat_history(st.session_state["chat_history"])
    #                 # st.rerun()
    #             # ëŒ€í™” ê¸°ë¡ ê´€ë¦¬
    #                 chat_history = st.session_state["chat_history"]
    #                 current_page = st.session_state["current_page"] - 1

    #                 if current_page < len(chat_history):
    #                     # ê¸°ì¡´ í˜ì´ì§€ ì—…ë°ì´íŠ¸
    #                     chat_history[current_page] = {"messages": st.session_state["messages"]}
    #                 else:
    #                     # ìƒˆë¡œìš´ í˜ì´ì§€ ì¶”ê°€
    #                     chat_history.append({"messages": st.session_state["messages"]})

    #                 # ëŒ€í™” ê¸°ë¡ ì €ì¥
    #                 save_chat_history(chat_history)

    #                 # UI ì—…ë°ì´íŠ¸
    #                 st.rerun()

    #         except Exception as e:
    #             st.error(f"An error occurred: {str(e)}")

    # if user_input:  # ì‚¬ìš©ìê°€ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ë©´
    #     # if user_input.strip():
    #     if user_input is not None and user_input.strip():
    #         try:
    #             response_lines = get_huggingface_response(st.session_state["model"], user_input)
    #             if response_lines:
    #                 # ë©”ì‹œì§€ ê¸°ë¡ ì¶”ê°€
    #                 st.session_state.messages.append({"role": "user", "content": user_input})
    #                 # st.session_state.messages.append({"role": "assistant", "content": "\n".join(response_lines)})
    #                 st.session_state.messages.append({"role": "assistant", "content": response_lines})

    #                 # Captum ê¸°ì—¬ë„ ê³„ì‚°
    #                 with st.spinner('Generating attribution heatmap...'):
    #                     heatmap_buffer = generate_heatmap(st.session_state["model"], user_input, response_lines)
                        
    #                     if heatmap_buffer:
    #                         # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    #                         timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    #                         temp_path = f"./heatmaps/heatmap_{timestamp}.png"
    #                         os.makedirs("./heatmaps", exist_ok=True)
                            
    #                         with open(temp_path, "wb") as f:
    #                             f.write(heatmap_buffer.getvalue())
                            
    #                         # íŒŒì¼ì„ base64ë¡œ ì¸ì½”ë”©
    #                         with open(temp_path, "rb") as f:
    #                             heatmap_base64 = base64.b64encode(f.read()).decode()
                            
    #                         # ì‘ë‹µê³¼ íˆíŠ¸ë§µì„ í•¨ê»˜ ë©”ì‹œì§€ì— ì €ì¥
    #                         st.session_state.messages.append({
    #                             "role": "assistant", 
    #                             "content": response_lines,
    #                             "heatmap": heatmap_base64
    #                         })
                            
    #                         # ì„ì‹œ íŒŒì¼ ì‚­ì œ
    #                         os.remove(temp_path)

    #                 # íˆíŠ¸ë§µ í‘œì‹œ
    #                 st.image(f"heatmaps/heatmap_{timestamp}.png", caption="Prompt Attribution Heatmap", use_column_width=True)

    #                 # ëŒ€í™” ê¸°ë¡ ê´€ë¦¬
    #                 chat_history = st.session_state["chat_history"]
    #                 current_page = st.session_state["current_page"] - 1

    #                 if current_page < len(chat_history):
    #                     # ê¸°ì¡´ í˜ì´ì§€ ì—…ë°ì´íŠ¸
    #                     chat_history[current_page] = {"messages": st.session_state["messages"]}
    #                 else:
    #                     # ìƒˆë¡œìš´ í˜ì´ì§€ ì¶”ê°€
    #                     chat_history.append({"messages": st.session_state["messages"]})

    #                 # ëŒ€í™” ê¸°ë¡ ì €ì¥
    #                 save_chat_history(chat_history)

    #                 # UI ì—…ë°ì´íŠ¸
    #                 st.rerun()

    #         except Exception as e:
    #             st.error(f"An error occurred: {str(e)}")
    if user_input:
        if user_input.strip():
            try:
                # LLM ì‘ë‹µ ìƒì„±
                response = get_huggingface_response(st.session_state["model"], user_input)
                if response:
                    # Captum ê¸°ì—¬ë„ ê³„ì‚°
                    heatmap_buffer = generate_heatmap(st.session_state["model"], user_input, response)

                    if heatmap_buffer:
                        # Streamlitì— ì´ë¯¸ì§€ í‘œì‹œ
                        st.markdown("### Prompt Attribution Heatmap")
                        st.image(heatmap_buffer, caption="Attribution Heatmap", use_column_width=True)

                        # ì±„íŒ… ë©”ì‹œì§€ ê¸°ë¡ì— ì¶”ê°€
                        heatmap_base64 = base64.b64encode(heatmap_buffer.getvalue()).decode("utf-8")
                        st.session_state.messages.append({
                            "role": "assistant",
                            "content": response,
                            "heatmap": heatmap_base64  # Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ì €ì¥
                        })

                        # ì±„íŒ… ê¸°ë¡ ì—…ë°ì´íŠ¸
                        chat_history = st.session_state["chat_history"]
                        current_page = st.session_state["current_page"] - 1

                        if current_page < len(chat_history):
                            chat_history[current_page] = {"messages": st.session_state["messages"]}
                        else:
                            chat_history.append({"messages": st.session_state["messages"]})

                        save_chat_history(chat_history)
                        st.rerun()
              
      


            except Exception as e:
                st.error(f"An error occurred: {str(e)}")